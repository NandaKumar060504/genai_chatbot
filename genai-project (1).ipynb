{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"****Phase-1****","metadata":{}},{"cell_type":"code","source":"%pip install pandas\n%pip install pyarrow\n%pip install fsspec\n%pip install huggingface_hub\n%pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:23:04.587854Z","iopub.execute_input":"2025-03-10T03:23:04.588044Z","iopub.status.idle":"2025-03-10T03:23:23.705844Z","shell.execute_reply.started":"2025-03-10T03:23:04.588026Z","shell.execute_reply":"2025-03-10T03:23:23.704731Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\nRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.4->pandas) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22.4->pandas) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22.4->pandas) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (19.0.1)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (2024.12.0)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.29.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.12.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2025.1.31)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_parquet(\"hf://datasets/CalebE/new_mental_health_conversations_all1/data/train-00000-of-00001.parquet\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:23:28.311170Z","iopub.execute_input":"2025-03-10T03:23:28.311473Z","iopub.status.idle":"2025-03-10T03:23:30.843000Z","shell.execute_reply.started":"2025-03-10T03:23:28.311449Z","shell.execute_reply":"2025-03-10T03:23:30.842291Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df_small = df.head(50)  # Change to 50 if needed\n\n# Save the reduced dataset (optional)\ndf_small.to_parquet(\"small_dataset.parquet\", index=False)\n\n# Display the reduced dataset\n# print(df_small)\n# print(df.head())\nprint(df.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:23:35.216775Z","iopub.execute_input":"2025-03-10T03:23:35.217325Z","iopub.status.idle":"2025-03-10T03:23:35.265891Z","shell.execute_reply.started":"2025-03-10T03:23:35.217282Z","shell.execute_reply":"2025-03-10T03:23:35.265214Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 13358 entries, 0 to 13357\nData columns (total 3 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   instruction  13358 non-null  object\n 1   output       13358 non-null  object\n 2   input        13358 non-null  object\ndtypes: object(3)\nmemory usage: 313.2+ KB\nNone\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nnltk.download(\"punkt\")\nnltk.download(\"stopwords\")\n\nstop_words = set(stopwords.words(\"english\"))\n\ndef clean_text(text):\n    if isinstance(text, str):\n        text = text.lower()  # Convert to lowercase\n        text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove special characters\n        tokens = word_tokenize(text)  # Tokenize\n        tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n        return \" \".join(tokens)  # Convert back to string\n    return text\n\ndf[\"clean_instruction\"] = df[\"instruction\"].apply(clean_text)\ndf[\"clean_input\"] = df[\"input\"].apply(clean_text)\ndf[\"clean_output\"] = df[\"output\"].apply(clean_text)\n\nprint(df[[\"clean_instruction\", \"clean_input\", \"clean_output\"]].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:23:37.432048Z","iopub.execute_input":"2025-03-10T03:23:37.432362Z","iopub.status.idle":"2025-03-10T03:23:56.611190Z","shell.execute_reply.started":"2025-03-10T03:23:37.432339Z","shell.execute_reply":"2025-03-10T03:23:56.610372Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n                                   clean_instruction  \\\n0         im feeling really anxious lately dont know   \n1                         think partner may cheating   \n2  im feeling really overwhelmed work school dont...   \n3  im trouble sleeping im constantly tired think ...   \n4        ive feeling really anxious lately dont know   \n\n                                         clean_input  \\\n0  licensed psychologist please provide patient h...   \n1  licensed psychologist please provide patient h...   \n2  licensed psychologist please provide patient h...   \n3  licensed psychologist please provide patient h...   \n4  licensed psychologist please provide patient h...   \n\n                                        clean_output  \n0  common feel anxious times many reasons recent ...  \n1  understandable feel worried suspicious situati...  \n2  sounds like youre going difficult time lets wo...  \n3  important talk doctor side effects youre exper...  \n4  common feel anxious without knowing exact caus...  \n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\n\n# Apply tokenization\ndf[\"tokenized_instruction\"] = df[\"clean_instruction\"].apply(word_tokenize)\ndf[\"tokenized_sentences\"] = df[\"clean_instruction\"].apply(sent_tokenize)\n\nprint(df[[\"tokenized_instruction\", \"tokenized_sentences\"]].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:24:00.772077Z","iopub.execute_input":"2025-03-10T03:24:00.772587Z","iopub.status.idle":"2025-03-10T03:24:02.355954Z","shell.execute_reply.started":"2025-03-10T03:24:00.772552Z","shell.execute_reply":"2025-03-10T03:24:02.355188Z"}},"outputs":[{"name":"stdout","text":"                               tokenized_instruction  \\\n0  [im, feeling, really, anxious, lately, dont, k...   \n1                    [think, partner, may, cheating]   \n2  [im, feeling, really, overwhelmed, work, schoo...   \n3  [im, trouble, sleeping, im, constantly, tired,...   \n4  [ive, feeling, really, anxious, lately, dont, ...   \n\n                                 tokenized_sentences  \n0       [im feeling really anxious lately dont know]  \n1                       [think partner may cheating]  \n2  [im feeling really overwhelmed work school don...  \n3  [im trouble sleeping im constantly tired think...  \n4      [ive feeling really anxious lately dont know]  \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef pos_tagging(text):\n    doc = nlp(text)\n    return [(token.text, token.pos_) for token in doc]\n\ndf[\"pos_tags\"] = df[\"clean_instruction\"].apply(pos_tagging)\n\nprint(df[[\"clean_instruction\", \"pos_tags\"]].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:24:05.643699Z","iopub.execute_input":"2025-03-10T03:24:05.644025Z","iopub.status.idle":"2025-03-10T03:25:36.960977Z","shell.execute_reply.started":"2025-03-10T03:24:05.643996Z","shell.execute_reply":"2025-03-10T03:25:36.960188Z"}},"outputs":[{"name":"stdout","text":"                                   clean_instruction  \\\n0         im feeling really anxious lately dont know   \n1                         think partner may cheating   \n2  im feeling really overwhelmed work school dont...   \n3  im trouble sleeping im constantly tired think ...   \n4        ive feeling really anxious lately dont know   \n\n                                            pos_tags  \n0  [(i, PRON), (m, AUX), (feeling, VERB), (really...  \n1  [(think, VERB), (partner, NOUN), (may, AUX), (...  \n2  [(i, PRON), (m, AUX), (feeling, VERB), (really...  \n3  [(i, PRON), (m, VERB), (trouble, NOUN), (sleep...  \n4  [(i, PRON), (ve, AUX), (feeling, VERB), (reall...  \n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from transformers import BertTokenizer, BertModel\nimport torch\n\n# Check if CUDA is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load model and tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = BertModel.from_pretrained(\"bert-base-uncased\").to(device)  # Move model to GPU\n\ndef get_bert_embedding(text):\n    tokens = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n    tokens = {key: value.to(device) for key, value in tokens.items()}  # Move input tensors to GPU\n    \n    with torch.no_grad():\n        output = model(**tokens)\n    \n    return output.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()  # Move output back to CPU for Pandas\n\n# Apply function to DataFrame\ndf[\"bert_embedding\"] = df[\"clean_instruction\"].apply(get_bert_embedding)\n\nprint(df[\"bert_embedding\"].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:26:02.152901Z","iopub.execute_input":"2025-03-10T03:26:02.153424Z","iopub.status.idle":"2025-03-10T03:28:15.829232Z","shell.execute_reply.started":"2025-03-10T03:26:02.153396Z","shell.execute_reply":"2025-03-10T03:28:15.828320Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b327909f74bf46a8b1aee848bcb6362b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de9f57e448da471c9d865b418458a976"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c4a1c3e491e43b3816539535347f95d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01330dfe9e5341338ffa323c6c8ec0a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b01c410650c476b8dcf24e502c68fdb"}},"metadata":{}},{"name":"stdout","text":"0    [0.04772697, 0.06270748, 0.39699778, 0.0469553...\n1    [0.06670084, -0.06419288, 0.087617114, -0.0953...\n2    [0.15714815, 0.38649198, 0.30842364, -0.025939...\n3    [-0.018088518, 0.32670245, 0.51900166, -0.2724...\n4    [0.06580624, 0.11805924, 0.31043625, 0.1069824...\nName: bert_embedding, dtype: object\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"df.to_parquet(\"processed_mental_health_data.parquet\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T13:02:07.674726Z","iopub.execute_input":"2025-03-07T13:02:07.675069Z","iopub.status.idle":"2025-03-07T13:02:08.435724Z","shell.execute_reply.started":"2025-03-07T13:02:07.675041Z","shell.execute_reply":"2025-03-07T13:02:08.434856Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"***Phase-2***","metadata":{}},{"cell_type":"code","source":"%pip install langchain\n%pip install langchain_community\n%pip install chromadb\n%pip install llama-cpp-python\n%pip install -U bitsandbytes\n%pip install faiss-gpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T04:54:02.461100Z","iopub.execute_input":"2025-03-10T04:54:02.461363Z","iopub.status.idle":"2025-03-10T04:56:25.018323Z","shell.execute_reply.started":"2025-03-10T04:54:02.461337Z","shell.execute_reply":"2025-03-10T04:56:25.017384Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.12)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.12)\nCollecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.25)\nRequirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.3)\nRequirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\nRequirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.11.0a2)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (4.12.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.29.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain) (3.0.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.22.4->langchain) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.22.4->langchain) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1.22.4->langchain) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2,>=1.22.4->langchain) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2,>=1.22.4->langchain) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\nDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\nInstalling collected packages: async-timeout\n  Attempting uninstall: async-timeout\n    Found existing installation: async-timeout 5.0.1\n    Uninstalling async-timeout-5.0.1:\n      Successfully uninstalled async-timeout-5.0.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed async-timeout-4.0.3\nNote: you may need to restart the kernel to use updated packages.\nCollecting langchain_community\n  Downloading langchain_community-0.3.19-py3-none-any.whl.metadata (2.4 kB)\nCollecting langchain-core<1.0.0,>=0.3.41 (from langchain_community)\n  Downloading langchain_core-0.3.43-py3-none-any.whl.metadata (5.9 kB)\nCollecting langchain<1.0.0,>=0.3.20 (from langchain_community)\n  Downloading langchain-0.3.20-py3-none-any.whl.metadata (7.7 kB)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.36)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.11.12)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (9.0.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.3)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\nCollecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain<1.0.0,>=0.3.20->langchain_community)\n  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<1.0.0,>=0.3.20->langchain_community) (2.11.0a2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (4.12.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.12)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain_community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain_community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain_community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain_community) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain_community) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain_community) (2.4.1)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (3.7.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain_community) (3.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain_community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain_community) (2.29.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.26.2->langchain_community) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.26.2->langchain_community) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3,>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3,>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.2.2)\nDownloading langchain_community-0.3.19-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\nDownloading langchain-0.3.20-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.43-py3-none-any.whl (415 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.4/415.4 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\nDownloading langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\nDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nInstalling collected packages: python-dotenv, httpx-sse, pydantic-settings, langchain-core, langchain-text-splitters, langchain, langchain_community\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.25\n    Uninstalling langchain-core-0.3.25:\n      Successfully uninstalled langchain-core-0.3.25\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.3.3\n    Uninstalling langchain-text-splitters-0.3.3:\n      Successfully uninstalled langchain-text-splitters-0.3.3\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.3.12\n    Uninstalling langchain-0.3.12:\n      Successfully uninstalled langchain-0.3.12\nSuccessfully installed httpx-sse-0.4.0 langchain-0.3.20 langchain-core-0.3.43 langchain-text-splitters-0.3.6 langchain_community-0.3.19 pydantic-settings-2.8.1 python-dotenv-1.0.1\nNote: you may need to restart the kernel to use updated packages.\nCollecting chromadb\n  Downloading chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\nCollecting build>=1.0.3 (from chromadb)\n  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.11.0a2)\nCollecting chroma-hnswlib==0.7.6 (from chromadb)\n  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\nCollecting fastapi>=0.95.2 (from chromadb)\n  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\nCollecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.4)\nCollecting posthog>=2.4.0 (from chromadb)\n  Downloading posthog-3.19.0-py2.py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\nCollecting onnxruntime>=1.14.1 (from chromadb)\n  Downloading onnxruntime-1.21.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.29.0)\nCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n  Downloading opentelemetry_exporter_otlp_proto_grpc-1.30.0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.51b0-py3-none-any.whl.metadata (2.2 kB)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.29.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.21.0)\nCollecting pypika>=0.48.9 (from chromadb)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.67.1)\nRequirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\nRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (5.13.0)\nRequirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.68.1)\nCollecting bcrypt>=4.0.1 (from chromadb)\n  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.1)\nCollecting kubernetes>=28.1.0 (from chromadb)\n  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (9.0.0)\nRequirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.2)\nCollecting mmh3>=4.0.1 (from chromadb)\n  Downloading mmh3-5.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\nRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.12)\nRequirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.28.1)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.9.4)\nRequirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.2)\nCollecting pyproject_hooks (from build>=1.0.3->chromadb)\n  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.2.1)\nCollecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\nRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\nRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\nRequirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\nRequirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\nCollecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.5->chromadb) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.5->chromadb) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.5->chromadb) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.5->chromadb) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.5->chromadb) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.5->chromadb) (2.4.1)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\nRequirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.15)\nRequirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\nCollecting opentelemetry-exporter-otlp-proto-common==1.30.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n  Downloading opentelemetry_exporter_otlp_proto_common-1.30.0-py3-none-any.whl.metadata (1.9 kB)\nCollecting opentelemetry-proto==1.30.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n  Downloading opentelemetry_proto-1.30.0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-sdk>=1.2.0 (from chromadb)\n  Downloading opentelemetry_sdk-1.30.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting protobuf (from onnxruntime>=1.14.1->chromadb)\n  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\nCollecting opentelemetry-instrumentation-asgi==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.51b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.51b0-py3-none-any.whl.metadata (6.3 kB)\nCollecting opentelemetry-semantic-conventions==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl.metadata (2.5 kB)\nCollecting opentelemetry-util-http==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.51b0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.0)\nCollecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb)\n  Downloading opentelemetry_api-1.30.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.29.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.29.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\nCollecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nRequirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\nCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading watchfiles-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.12.0)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.5->chromadb) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22.5->chromadb) (2024.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22.5->chromadb) (2024.2.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\nDownloading chromadb-0.6.3-py3-none-any.whl (611 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\nDownloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mmh3-5.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.21.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.30.0-py3-none-any.whl (18 kB)\nDownloading opentelemetry_exporter_otlp_proto_common-1.30.0-py3-none-any.whl (18 kB)\nDownloading opentelemetry_proto-1.30.0-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.51b0-py3-none-any.whl (12 kB)\nDownloading opentelemetry_instrumentation-0.51b0-py3-none-any.whl (30 kB)\nDownloading opentelemetry_instrumentation_asgi-0.51b0-py3-none-any.whl (16 kB)\nDownloading opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl (177 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_api-1.30.0-py3-none-any.whl (64 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_util_http-0.51b0-py3-none-any.whl (7.3 kB)\nDownloading opentelemetry_sdk-1.30.0-py3-none-any.whl (118 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading posthog-3.19.0-py2.py3-none-any.whl (77 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading durationpy-0.9-py3-none-any.whl (3.5 kB)\nDownloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nDownloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading starlette-0.46.1-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading watchfiles-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\nDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53769 sha256=a9113a81ea7f8eb3c3f6ffeb413f83d36d932c9a128e1e45386254abe59e04dc\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\nSuccessfully built pypika\nInstalling collected packages: pypika, monotonic, durationpy, uvloop, uvicorn, pyproject_hooks, protobuf, opentelemetry-util-http, mmh3, humanfriendly, httptools, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-proto, opentelemetry-api, coloredlogs, build, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, onnxruntime, chroma-hnswlib, chromadb\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: opentelemetry-api\n    Found existing installation: opentelemetry-api 1.29.0\n    Uninstalling opentelemetry-api-1.29.0:\n      Successfully uninstalled opentelemetry-api-1.29.0\n  Attempting uninstall: opentelemetry-semantic-conventions\n    Found existing installation: opentelemetry-semantic-conventions 0.50b0\n    Uninstalling opentelemetry-semantic-conventions-0.50b0:\n      Successfully uninstalled opentelemetry-semantic-conventions-0.50b0\n  Attempting uninstall: opentelemetry-sdk\n    Found existing installation: opentelemetry-sdk 1.29.0\n    Uninstalling opentelemetry-sdk-1.29.0:\n      Successfully uninstalled opentelemetry-sdk-1.29.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.3 which is incompatible.\ngoogle-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.3 which is incompatible.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\ntensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.6.3 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.11 httptools-0.6.4 humanfriendly-10.0 kubernetes-32.0.1 mmh3-5.1.0 monotonic-1.6 onnxruntime-1.21.0 opentelemetry-api-1.30.0 opentelemetry-exporter-otlp-proto-common-1.30.0 opentelemetry-exporter-otlp-proto-grpc-1.30.0 opentelemetry-instrumentation-0.51b0 opentelemetry-instrumentation-asgi-0.51b0 opentelemetry-instrumentation-fastapi-0.51b0 opentelemetry-proto-1.30.0 opentelemetry-sdk-1.30.0 opentelemetry-semantic-conventions-0.51b0 opentelemetry-util-http-0.51b0 posthog-3.19.0 protobuf-5.29.3 pypika-0.48.9 pyproject_hooks-1.2.0 starlette-0.46.1 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.4\nNote: you may need to restart the kernel to use updated packages.\nCollecting llama-cpp-python\n  Downloading llama_cpp_python-0.3.7.tar.gz (66.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\nRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.26.4)\nCollecting diskcache>=5.6.1 (from llama-cpp-python)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.20.0->llama-cpp-python) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.7-cp310-cp310-linux_x86_64.whl size=4601139 sha256=68c0080b350a4b159317a159a2615c5717c47c5d281a03ff8c04383a22db403f\n  Stored in directory: /root/.cache/pip/wheels/5c/8f/58/a39eb13258f3bbf64bb36ed76d31979579a6f175be38de06b7\nSuccessfully built llama-cpp-python\nInstalling collected packages: diskcache, llama-cpp-python\nSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.3.7\nNote: you may need to restart the kernel to use updated packages.\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.3\nNote: you may need to restart the kernel to use updated packages.\nCollecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom langchain_community.llms import HuggingFacePipeline\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.document_loaders import TextLoader\nfrom langchain.prompts import PromptTemplate\nfrom transformers import pipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T04:56:52.050234Z","iopub.execute_input":"2025-03-10T04:56:52.050913Z","iopub.status.idle":"2025-03-10T04:56:52.055569Z","shell.execute_reply.started":"2025-03-10T04:56:52.050880Z","shell.execute_reply":"2025-03-10T04:56:52.054675Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport pandas as pd\n# Download necessary NLTK dat\nnltk.download(\"punkt\")\nnltk.download(\"stopwords\")\n\ndf = pd.read_parquet(\"hf://datasets/CalebE/new_mental_health_conversations_all1/data/train-00000-of-00001.parquet\")\n\n# ✅ Merge `input` and `output` to create meaningful text for retrieval\ndf[\"combined_text\"] = df[\"instruction\"].fillna(\"\") + \" \" + df[\"input\"].fillna(\"\") + \" \" + df[\"output\"].fillna(\"\")\n\n# ✅ Remove empty texts\ndf = df.dropna(subset=[\"combined_text\"])\n\n# ✅ Convert into LangChain Document format\ndocuments = [Document(page_content=text) for text in df[\"combined_text\"].tolist()]\n\n# ✅ Split documents into chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\ntexts = text_splitter.split_documents(documents)\n\n\n# Define stopwords\nstop_words = set(stopwords.words(\"english\"))\n\n# Define text-cleaning function\ndef clean_text(text):\n    if isinstance(text, str):\n        text = text.lower()  # Convert to lowercase\n        text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove special characters\n        tokens = word_tokenize(text)  # Tokenize\n        tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n        return \" \".join(tokens)  # Convert back to string\n    return text\n\n# Apply cleaning\ndf[\"clean_input\"] = df[\"input\"].apply(clean_text)\ndf[\"clean_output\"] = df[\"output\"].apply(clean_text)\ndf[\"clean_instruction\"] = df[\"instruction\"].apply(clean_text)\n\n# Check processed data\nprint(df[[\"clean_instruction\", \"clean_input\", \"clean_output\"]].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T04:59:11.572060Z","iopub.execute_input":"2025-03-10T04:59:11.572350Z","iopub.status.idle":"2025-03-10T04:59:31.977367Z","shell.execute_reply.started":"2025-03-10T04:59:11.572328Z","shell.execute_reply":"2025-03-10T04:59:31.976377Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n                                   clean_instruction  \\\n0         im feeling really anxious lately dont know   \n1                         think partner may cheating   \n2  im feeling really overwhelmed work school dont...   \n3  im trouble sleeping im constantly tired think ...   \n4        ive feeling really anxious lately dont know   \n\n                                         clean_input  \\\n0  licensed psychologist please provide patient h...   \n1  licensed psychologist please provide patient h...   \n2  licensed psychologist please provide patient h...   \n3  licensed psychologist please provide patient h...   \n4  licensed psychologist please provide patient h...   \n\n                                        clean_output  \n0  common feel anxious times many reasons recent ...  \n1  understandable feel worried suspicious situati...  \n2  sounds like youre going difficult time lets wo...  \n3  important talk doctor side effects youre exper...  \n4  common feel anxious without knowing exact caus...  \n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import Chroma\nimport chromadb\nimport os\n\n# Define storage path\nDB_DIR = os.path.join(os.getcwd(), \"mental_health_db\")\n\n# Load embeddings model\nembedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Convert text into document format\nfrom langchain.docstore.document import Document\n\ndocuments = [Document(page_content=text) for text in df[\"clean_instruction\"]]\n\n# Store in ChromaDB\nvectorstore = Chroma.from_documents(\n    documents,\n    embedding_model,\n    persist_directory=DB_DIR\n)\n\n# Persist the database\nvectorstore.persist()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T04:59:40.271521Z","iopub.execute_input":"2025-03-10T04:59:40.271845Z","iopub.status.idle":"2025-03-10T04:59:57.023947Z","shell.execute_reply.started":"2025-03-10T04:59:40.271822Z","shell.execute_reply":"2025-03-10T04:59:57.022960Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-8-b1f87fd0dd91>:25: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n  vectorstore.persist()\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5})\n\nquery = \"How can I manage anxiety?\"\nretrieved_docs = retriever.get_relevant_documents(query)\n\n# Display retrieved docs\nfor i, doc in enumerate(retrieved_docs):\n    print(f\"Document {i+1}:\\n{doc.page_content}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T05:00:06.124233Z","iopub.execute_input":"2025-03-10T05:00:06.124689Z","iopub.status.idle":"2025-03-10T05:00:06.178338Z","shell.execute_reply.started":"2025-03-10T05:00:06.124635Z","shell.execute_reply":"2025-03-10T05:00:06.177354Z"}},"outputs":[{"name":"stdout","text":"Document 1:\nim really struggling anxiety manage\n\nDocument 2:\nim really struggling anxiety interfering daily life manage\n\nDocument 3:\nim struggling anxiety social situations avoiding altogether manage anxiety\n\nDocument 4:\nim trouble anxiety dont want take medication manage anxiety without medication\n\nDocument 5:\nfeel like cant control anxiety\n\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-9-dc03a6aa411a>:4: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n  retrieved_docs = retriever.get_relevant_documents(query)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"** Chain-of-Thought (CoT) Prompting**","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    device_map=\"auto\",  # Offload intelligently (some layers to CPU if needed)\n    torch_dtype=\"auto\",  # Use automatic precision\n    max_new_tokens=512,  # Reduce max tokens\n    temperature=0.75,\n    top_p=1\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T05:00:12.615960Z","iopub.execute_input":"2025-03-10T05:00:12.616243Z","iopub.status.idle":"2025-03-10T05:00:12.650708Z","shell.execute_reply.started":"2025-03-10T05:00:12.616222Z","shell.execute_reply":"2025-03-10T05:00:12.649678Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-11bf5d0a64f6>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m pipe = pipeline(\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Offload intelligently (some layers to CPU if needed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"llm = HuggingFacePipeline(pipeline=pipe)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T13:40:07.668335Z","iopub.execute_input":"2025-03-09T13:40:07.668700Z","iopub.status.idle":"2025-03-09T13:40:07.684545Z","shell.execute_reply.started":"2025-03-09T13:40:07.668672Z","shell.execute_reply":"2025-03-09T13:40:07.683616Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-14-b97a330b5f24>:1: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n  llm = HuggingFacePipeline(pipeline=pipe)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom langchain_community.llms import HuggingFacePipeline\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.docstore.document import Document\n\n# ✅ Load dataset (Parquet file)\ndf = pd.read_parquet(\"hf://datasets/CalebE/new_mental_health_conversations_all1/data/train-00000-of-00001.parquet\")\n\n# ✅ Merge `input` and `output` to create meaningful text for retrieval\ndf[\"combined_text\"] = df[\"instruction\"].fillna(\"\") + \" \" + df[\"input\"].fillna(\"\") + \" \" + df[\"output\"].fillna(\"\")\n\n# ✅ Remove empty texts\ndf = df.dropna(subset=[\"combined_text\"])\n\n# ✅ Convert into LangChain Document format\ndocuments = [Document(page_content=text) for text in df[\"combined_text\"].tolist()]\n\n# ✅ Split documents into chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\ntexts = text_splitter.split_documents(documents)\n\n# ✅ Create embeddings & store in FAISS\nembedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\nvectorstore = FAISS.from_documents(texts, embedding_model)\nretriever = vectorstore.as_retriever()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T05:00:20.960157Z","iopub.execute_input":"2025-03-10T05:00:20.960595Z","iopub.status.idle":"2025-03-10T05:00:53.816828Z","shell.execute_reply.started":"2025-03-10T05:00:20.960558Z","shell.execute_reply":"2025-03-10T05:00:53.816128Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T05:01:12.286767Z","iopub.execute_input":"2025-03-10T05:01:12.287047Z","iopub.status.idle":"2025-03-10T05:01:12.290827Z","shell.execute_reply.started":"2025-03-10T05:01:12.287026Z","shell.execute_reply":"2025-03-10T05:01:12.289954Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,  # Use 16-bit precision\n    device_map=\"auto\"  # Automatically offload to CPU/GPU\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T05:01:15.325510Z","iopub.execute_input":"2025-03-10T05:01:15.325832Z","iopub.status.idle":"2025-03-10T05:03:19.591282Z","shell.execute_reply.started":"2025-03-10T05:01:15.325808Z","shell.execute_reply":"2025-03-10T05:03:19.590677Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/638 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f94f24050e84952b328b0dc497d0588"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ebb64a2ba96497d8682e1411de996ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49ab6d009c88418195dc111f6cef76a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00008.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"124ca10eb7274551b40822270c7aab35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"517ad12e50664a32819badd79bf57e2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d6eadf2bc7b4067aec6b912db85d918"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d25e21dc4a8f42c2bcf794cfcdeb895f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd70ced9f2b54134bec73ba77f7c6d4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d4911ca3e4447cdabc906593956f274"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00007-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93fb874cc0ef484aa1a97f79ddc8303b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00008-of-00008.safetensors:   0%|          | 0.00/816M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7981f6638e9f4e9ab0edf8d51ba0700d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a262409c295a4c74922f3046011049d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d178cdbc3ced4256ac7a298e11ac16d1"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# ✅ Load tokenizer & model\n# model_name = \"HuggingFaceH4/zephyr-7b-beta\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n# model = AutoModelForCausalLM.from_pretrained(model_name)\n\n# ✅ Create text generation pipeline\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,  # Reduce for memory efficiency\n    temperature=0.75,\n    top_p=1,\n    device_map=\"auto\"  # Enable GPU/CPU offloading\n)\n\n# ✅ Wrap the pipeline in LangChain's HuggingFacePipeline LLM\nllm = HuggingFacePipeline(pipeline=pipe)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T05:03:34.410411Z","iopub.execute_input":"2025-03-10T05:03:34.410754Z","iopub.status.idle":"2025-03-10T05:03:36.010433Z","shell.execute_reply.started":"2025-03-10T05:03:34.410728Z","shell.execute_reply":"2025-03-10T05:03:36.009693Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"787f9a83ecb94c84a9ac374df7668f40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddb1df80bcb34e17a744108581e2e337"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78cf51671bdd487a88a73ad2bb0cfd18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7488876ccd16411a9c02f28992b24a8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a044be310254c268c198b10bf74ad71"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n<ipython-input-16-e155b5058dad>:18: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n  llm = HuggingFacePipeline(pipeline=pipe)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom langchain_community.llms import HuggingFacePipeline\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.docstore.document import Document\n\n# ✅ Define Model Name Once\nmodel_name = \"HuggingFaceH4/zephyr-7b-beta\"\n\n# ✅ Load tokenizer & model (Only Once)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,  # Use 16-bit precision\n    device_map=\"auto\"  # Automatically offload to CPU/GPU\n)\n\n# ✅ Create text generation pipeline\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,  # Reduce for memory efficiency\n    temperature=0.75,\n    top_p=1,\n    device_map=\"auto\"  # Enable GPU/CPU offloading\n)\n\n# ✅ Wrap the pipeline in LangChain's HuggingFacePipeline LLM\nllm = HuggingFacePipeline(pipeline=pipe)\n\n# ✅ Load dataset (Parquet file)\ndf = pd.read_parquet(\"hf://datasets/CalebE/new_mental_health_conversations_all1/data/train-00000-of-00001.parquet\")\n\n# ✅ Merge `input` and `output` to create meaningful text for retrieval\ndf[\"combined_text\"] = df[\"instruction\"].fillna(\"\") + \" \" + df[\"input\"].fillna(\"\") + \" \" + df[\"output\"].fillna(\"\")\n\n# ✅ Remove empty texts\ndf = df.dropna(subset=[\"combined_text\"])\n\n# ✅ Convert into LangChain Document format\ndocuments = [Document(page_content=text) for text in df[\"combined_text\"].tolist()]\n\n# ✅ Split documents into chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\ntexts = text_splitter.split_documents(documents)\n\n# ✅ Create embeddings & store in FAISS\nembedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\nvectorstore = FAISS.from_documents(texts, embedding_model)\nretriever = vectorstore.as_retriever()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T05:07:27.231131Z","iopub.execute_input":"2025-03-10T05:07:27.231475Z","iopub.status.idle":"2025-03-10T05:08:23.136236Z","shell.execute_reply.started":"2025-03-10T05:07:27.231423Z","shell.execute_reply":"2025-03-10T05:08:23.135266Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53bf0cd2bab5425a9809b54f9e325bf5"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\n\n# ✅ Define Chain of Thought Prompt Template\ncot_prompt_template = \"\"\"You are an AI assistant that provides detailed and step-by-step responses based on retrieved information. \nUse logical reasoning to analyze the given context and answer the question in a structured manner.\n\nContext:\n{context}\n\nFollow these steps:\n1. Identify key details from the context.\n2. Break down the reasoning step by step.\n3. Provide a detailed, clear, and informative answer.\n\nQuestion: {question}\nFinal Answer:\"\"\"\n\ncot_prompt = PromptTemplate(\n    template=cot_prompt_template, \n    input_variables=[\"context\", \"question\"]\n)\n\n# ✅ Create Retrieval QA Chain with CoT Prompting\nretrieval_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",  # Stuffing all retrieved docs in the prompt\n    retriever=retriever,\n    chain_type_kwargs={\"prompt\": cot_prompt}\n)\n\n# ✅ Query with structured input\nquery = \"How does stress impact mental health?\"\nresponse = retrieval_chain.invoke({\"query\": query})  \n\n# ✅ Beautify the output\ndef beautify_output(response):\n    print(\"\\n=== Final Answer ===\\n\")\n    print(response[\"result\"])\n    # print(\"\\n=== Source Documents ===\")\n    # for i, doc in enumerate(response[\"source_documents\"], 1):\n    #     print(f\"\\n📌 Source {i}: {doc.page_content}\")\n\n# ✅ Display the formatted response\nbeautify_output(response)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T05:11:04.476280Z","iopub.execute_input":"2025-03-10T05:11:04.476615Z","iopub.status.idle":"2025-03-10T05:11:14.056347Z","shell.execute_reply.started":"2025-03-10T05:11:04.476591Z","shell.execute_reply":"2025-03-10T05:11:14.055622Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n=== Final Answer ===\n\nYou are an AI assistant that provides detailed and step-by-step responses based on retrieved information. \nUse logical reasoning to analyze the given context and answer the question in a structured manner.\n\nContext:\nengaging in stress-reducing activities such as exercise or meditation can help improve your overall mental health.\n\nissues that may be contributing to your stress levels.\n\nbe helpful to explore any underlying issues that may be contributing to your stress.\n\nIt's important to identify the source of your stress and develop a plan to manage it.\n\nFollow these steps:\n1. Identify key details from the context.\n2. Break down the reasoning step by step.\n3. Provide a detailed, clear, and informative answer.\n\nQuestion: How does stress impact mental health?\nFinal Answer: Stress, when experienced in short bursts, can be beneficial as it helps us respond to challenging situations. However, chronic stress can have negative effects on our mental health. Prolonged stress can lead to anxiety, depression, and other mental health disorders. Engaging in stress-reducing activities such as exercise or meditation can help improve overall mental health by reducing the levels of stress hormones in the body and promoting relaxation. It's important to identify the source of stress and develop a plan to manage it, as this can help prevent the negative impacts of chronic stress on mental health. By exploring any underlying issues that may be contributing to stress, individuals can develop coping strategies and find healthy ways to manage stress.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\n\n# ✅ Define Chain of Thought (CoT) Prompt\ncot_prompt_template = \"\"\"You are an AI assistant that provides detailed, structured, and step-by-step responses based on retrieved information.\nUse logical reasoning to analyze the given context and answer the question effectively.\n\nContext:\n{context}\n\n**Step-by-Step Breakdown:**\n1. Identify key details from the context.\n2. Break down the reasoning step by step.\n3. Provide a detailed, clear, and informative answer.\n\n**Question:** {question}\n**Final Answer:**\"\"\"\n\ncot_prompt = PromptTemplate(\n    template=cot_prompt_template, \n    input_variables=[\"context\", \"question\"]\n)\n\n# ✅ Define Tree of Thoughts (ToT) Prompt\ntot_prompt_template = \"\"\"You are an AI assistant that considers multiple reasoning paths before answering.\nUse a Tree of Thoughts approach to explore different possible explanations.\n\nContext:\n{context}\n\n**Reasoning Paths (Tree of Thoughts):**\n1. **Branch 1:** Consider one possible explanation and analyze it.\n2. **Branch 2:** Consider an alternative viewpoint and explore it.\n3. **Branch 3:** If applicable, weigh both perspectives and decide the best response.\n\n**Question:** {question}\n**Final Answer (Based on Best Reasoning Path):**\"\"\"\n\ntot_prompt = PromptTemplate(\n    template=tot_prompt_template, \n    input_variables=[\"context\", \"question\"]\n)\n\n# ✅ Define Graph of Thoughts (GoT) Prompt\ngot_prompt_template = \"\"\"You are an AI assistant that connects multiple reasoning paths into a structured answer.\nUse a Graph of Thoughts approach to analyze different perspectives and combine them for the best response.\n\nContext:\n{context}\n\n**Graph of Thoughts Analysis:**\n1. Identify the key ideas from the retrieved documents.\n2. Find connections between different concepts.\n3. Integrate multiple insights into a cohesive and well-structured response.\n\n**Question:** {question}\n**Final Answer (Combining Multiple Insights):**\"\"\"\n\ngot_prompt = PromptTemplate(\n    template=got_prompt_template, \n    input_variables=[\"context\", \"question\"]\n)\n\n# ✅ Create Retrieval Chains for CoT, ToT, and GoT\nretrieval_chain_cot = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",  \n    retriever=retriever,\n    chain_type_kwargs={\"prompt\": cot_prompt}\n)\n\nretrieval_chain_tot = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=retriever,\n    chain_type_kwargs={\"prompt\": tot_prompt}\n)\n\nretrieval_chain_got = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=retriever,\n    chain_type_kwargs={\"prompt\": got_prompt}\n)\n\n# ✅ Query and Get Responses\nquery = \"How does stress impact mental health?\"\n\nresponse_cot = retrieval_chain_cot.invoke({\"query\": query})\nresponse_tot = retrieval_chain_tot.invoke({\"query\": query})\nresponse_got = retrieval_chain_got.invoke({\"query\": query})\n\n# ✅ Beautify the output\ndef beautify_output(response, method_name):\n    print(f\"\\n=== {method_name} Final Answer ===\\n\")\n    print(response[\"result\"])\n    print(\"\\n=== Source Documents ===\")\n    for i, doc in enumerate(response[\"source_documents\"], 1):\n        print(f\"\\n📌 Source {i}: {doc.page_content}\")\n\n# ✅ Display the responses\nbeautify_output(response_cot, \"Chain of Thought (CoT)\")\nbeautify_output(response_tot, \"Tree of Thoughts (ToT)\")\nbeautify_output(response_got, \"Graph of Thoughts (GoT)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}